# ğŸ§  Autoencoder from Scratch on MNIST

This project demonstrates how to build and train an **Autoencoder** from scratch using the **MNIST dataset**. The objective is to compress and reconstruct handwritten digits, learning efficient latent representations.

---

## ğŸ§¾ Whatâ€™s Inside?

- âœ… Fully connected **Autoencoder** implemented from scratch
- âœ… Trained on **MNIST handwritten digit dataset**
- âœ… Visual comparison of original and reconstructed digits
- âœ… Code built using **TensorFlow/Keras** (no pre-built autoencoder APIs)
- âœ… Visualization of the latent space (optional: t-SNE or PCA)

---

## ğŸ“Š Dataset

- **MNIST**: 70,000 grayscale images of handwritten digits (28x28)
- Train set: 60,000 samples  
- Test set: 10,000 samples  
- Each pixel is normalized to [0, 1]

---

## ğŸ—ï¸ Model Architecture

### Encoder:
- Input: 784 (flattened 28x28 image)
- Dense(256) â†’ Sigmoid  
- Dense(128) â†’ Sigmoid 
- Latent Vector: 32 units

### Decoder:
- Dense(128) â†’ Sigmoid 
- Dense(256) â†’ Sigmoid 
- Output: 784 (reshaped to 28x28) â†’ Sigmoid

---
